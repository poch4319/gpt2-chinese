{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.nn import DataParallel\n",
    "from tokenizations.bpe_tokenizer import get_encoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預處理資料集\n",
    "- 把初始句和回答結合成一個文本 (data point)\n",
    "- 只保留回答的情緒 (在 prediction 時該情緒可以用來引導情緒回達)\n",
    "- 載入 tokenizer 然後加入新的情緒 token\n",
    "- 用 tokenizer 把文本轉換成 ids 並存檔\n",
    "- 存檔加入新情緒 token 的 tokenizer\n",
    "- dataset : https://www.biendata.xyz/ccf_tcci2018/datasets/ecg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "s2t.json Simplified Chinese to Traditional Chinese 簡體到繁體\n",
    "t2s.json Traditional Chinese to Simplified Chinese 繁體到簡體\n",
    "s2tw.json Simplified Chinese to Traditional Chinese (Taiwan Standard) 簡體到臺灣正體\n",
    "tw2s.json Traditional Chinese (Taiwan Standard) to Simplified Chinese 臺灣正體到簡體\n",
    "s2hk.json Simplified Chinese to Traditional Chinese (Hong Kong variant) 簡體到香港繁體\n",
    "hk2s.json Traditional Chinese (Hong Kong variant) to Simplified Chinese 香港繁體到簡體\n",
    "s2twp.json Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom 簡體到繁體（臺灣正體標準）並轉換爲臺灣常用詞彙\n",
    "tw2sp.json Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom 繁體（臺灣正體標準）到簡體並轉換爲中國大陸常用詞彙\n",
    "t2tw.json Traditional Chinese (OpenCC Standard) to Taiwan Standard 繁體（OpenCC 標準）到臺灣正體\n",
    "hk2t.json Traditional Chinese (Hong Kong variant) to Traditional Chinese 香港繁體到繁體（OpenCC 標準）\n",
    "t2hk.json Traditional Chinese (OpenCC Standard) to Hong Kong variant 繁體（OpenCC 標準）到香港繁體\n",
    "t2jp.json Traditional Chinese Characters (Kyūjitai) to New Japanese Kanji (Shinjitai) 繁體（OpenCC 標準，舊字體）到日文新字體\n",
    "jp2t.json New Japanese Kanji (Shinjitai) to Traditional Chinese Characters (Kyūjitai) 日文新字體到繁體（OpenCC 標準，舊字體）\n",
    "tw2t.json Traditional Chinese (Taiwan standard) to Traditional Chinese 臺灣正體到繁體（OpenCC 標準）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opencc\n",
    "emotion_dict = {0: '[其他]', \\\n",
    "                1: '[喜歡]', \\\n",
    "                2: '[悲傷]', \\\n",
    "                3: '[噁心]', \\\n",
    "                4: '[憤怒]', \\\n",
    "                5: '[喜樂]'}\n",
    "\n",
    "converter = opencc.OpenCC('s2t.json')\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "with open(\"data/ecg_train_data.json\", encoding=\"utf-8\") as f:\n",
    "    lines = json.load(f)\n",
    "    processed = []\n",
    "    for line in lines:\n",
    "        post = converter.convert(line[0][0]).strip() # 起始句轉成繁體, 丟掉情緒標籤\n",
    "        emo = emotion_dict[line[1][1]] #回答句的情緒標籤\n",
    "        reply = converter.convert(line[1][0]).strip() #回答句標籤轉成中文放在句首，並轉成繁體\n",
    "        processed.extend([post + emo + reply])\n",
    "        #print(line)\n",
    "\n",
    "with open(\"data/ecg_train_data_processed.json\", 'w') as fi:\n",
    "    json.dump(processed, fi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ecg_train_data_processed.json\", encoding=\"utf-8\") as f:\n",
    "    lines = json.load(f)\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料轉換成 token ids 並儲存\n",
    "def build_files(data_path, tokenized_data_path, num_pieces, full_tokenizer, min_length):\n",
    "    #num_pieces 将训练语料分成多少份\n",
    "    with open(data_path, 'r', encoding='utf8') as f:\n",
    "        print('reading lines')\n",
    "        lines = json.load(f)\n",
    "        lines = [line.replace('\\n', ' [SEP] ') for line in lines]  # 用[SEP]表示换行, 段落之间使用SEP表示段落结束\n",
    "    all_len = len(lines)\n",
    "    if not os.path.exists(tokenized_data_path):\n",
    "        os.mkdir(tokenized_data_path)\n",
    "    for i in tqdm(range(num_pieces)):\n",
    "        sublines = lines[all_len // num_pieces * i: all_len // num_pieces * (i + 1)]\n",
    "        if i == num_pieces - 1:\n",
    "            sublines.extend(lines[all_len // num_pieces * (i + 1):])  # 把尾部例子添加到最后一个piece\n",
    "        sublines = [full_tokenizer.tokenize(line) for line in sublines if\n",
    "                    len(line) > min_length]  # 只考虑长度超过min_length的句子\n",
    "        sublines = [full_tokenizer.convert_tokens_to_ids(line) for line in sublines]\n",
    "        full_line = []\n",
    "        for subline in sublines:\n",
    "            full_line.append(full_tokenizer.convert_tokens_to_ids('[MASK]'))  # 文章开头添加MASK表示文章开始\n",
    "            full_line.extend(subline)\n",
    "            full_line.append(full_tokenizer.convert_tokens_to_ids('[CLS]'))  # 文章之间添加CLS表示文章结束\n",
    "        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'w') as f:\n",
    "            for id in full_line:\n",
    "                f.write(str(id) + ' ')\n",
    "    print('finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## add special tokens\n",
    "from tokenizations import tokenization_bert_word_level as tokenization_bert\n",
    "added_tokens = {'additional_special_tokens':['[其他]', '[喜歡]', '[悲傷]', '[噁心]', '[憤怒]', '[喜樂]']}\n",
    "\n",
    "full_tokenizer = tokenization_bert.BertTokenizer(vocab_file='pretrained_model/vocab.txt')  \n",
    "full_tokenizer.add_special_tokens(added_tokens)   #要把 additional_special_tokens 這個 Key 加入 list of your special tokens, 其他例如 cls 他本身就有排好 cls key了\n",
    "\n",
    "\n",
    "#model.resize_token_embeddings(len(full_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[其他]', '[喜歡]', '[悲傷]', '[噁心]', '[憤怒]', '[喜樂]']\n",
      "[21128, 21129, 21130, 21131, 21132, 21133]\n"
     ]
    }
   ],
   "source": [
    "## test the added speical tokens\n",
    "print(full_tokenizer.additional_special_tokens)\n",
    "print(full_tokenizer.additional_special_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tokenizer.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save updated tokenizer\n",
    "\n",
    "import torch\n",
    "torch.save(full_tokenizer, \"manmade/tokenizer.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 5\u001b[0m build_files(\u001b[39m\"\u001b[39m\u001b[39mdata/ecg_train_data_processed.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m100\u001b[39m, full_tokenizer, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_files' is not defined"
     ]
    }
   ],
   "source": [
    "## build file for training\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "build_files(\"data/ecg_train_data_processed.json\", \"data/\", 100, full_tokenizer, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "- load pretrained model\n",
    "- expand embedding dimensions for added special tokens\n",
    "- forward to training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "full_tokenizer = torch.load(\"manmade/tokenizer.ckpt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21134, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "pretrained_model_loc = \"./pretrained_model/\"\n",
    "#model = transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.from_pretrained(config='./pretrained_model/config.json', \n",
    "#                                                                            pretrained_weights='./pretrained_model/pytorch_model.bin', \n",
    "#                                                                            vocab_file='./pretrained_model/vocab.txt')\n",
    "model = transformers.modeling_gpt2.GPT2LMHeadModel.from_pretrained(\"./pretrained_model/\")\n",
    "model.resize_token_embeddings(len(full_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "{\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 400\n",
      "    }\n",
      "  },\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "n_ctx: 1024\n"
     ]
    }
   ],
   "source": [
    "model_config = transformers.modeling_gpt2.GPT2Config.from_json_file(\"./pretrained_model/config.json\")\n",
    "print('config:\\n' + model_config.to_json_string())\n",
    "n_ctx = model_config.n_ctx\n",
    "print(f'n_ctx: {n_ctx}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- 然後 model 的 vocab 記得要先擴充，因為 tokenizer 擴充了 而且辭典也擴充了 \n",
    "- 新的 vocab 應該也要存在某個地方，這樣新 init 的 tokenizer 才能讀取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21134, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "num_pieces = 100\n",
    "batch_size = 8\n",
    "stride = 768 #训练时取训练数据的窗口步长\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "multi_gpu = False\n",
    "gradient_accumulation = 1 #'梯度积累'\n",
    "\n",
    "full_len = 0\n",
    "total_steps = int(full_len / stride * epochs / batch_size / gradient_accumulation)\n",
    "lr =1.5e-4\n",
    "warmup_steps = 2000\n",
    "\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=lr, correct_bias=True)\n",
    "scheduler = transformers.WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps,\n",
    "                                                        t_total=total_steps)\n",
    "fp16 = False # 混合精度 # 不支持半精度的显卡请勿打开\n",
    "overall_step = 0\n",
    "running_loss = 0\n",
    "#tb_writer = SummaryWriter(log_dir=args.writer_dir)\n",
    "log_step = 1 #'多少步汇报一次loss，设置为gradient accumulation的整数倍'\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "output_dir = 'saved_model/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "tokenized_data_path = 'data/'\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "time: 2023-02-09 11:46:13.855805\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'running_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m#  optimizer step\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m (overall_step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m gradient_accumulation \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 52\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     53\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     54\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'running_loss' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    now = datetime.now()\n",
    "    print('time: {}'.format(now))\n",
    "    x = np.linspace(0, num_pieces - 1, num_pieces, dtype=np.int32)\n",
    "    random.shuffle(x)\n",
    "    piece_num = 0\n",
    "    for i in x:\n",
    "        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'r') as f:\n",
    "            line = f.read().strip()\n",
    "        tokens = line.split()\n",
    "        tokens = [int(token) for token in tokens]\n",
    "        start_point = 0\n",
    "        samples = []\n",
    "        while start_point < len(tokens) - n_ctx:\n",
    "            samples.append(tokens[start_point: start_point + n_ctx])\n",
    "            start_point += stride\n",
    "        if start_point < len(tokens):\n",
    "            samples.append(tokens[len(tokens)-n_ctx:])\n",
    "        random.shuffle(samples)\n",
    "        for step in range(len(samples) // batch_size):  # drop last\n",
    "\n",
    "            #  prepare data\n",
    "            batch = samples[step * batch_size: (step + 1) * batch_size]\n",
    "            batch_inputs = []\n",
    "            for ids in batch:\n",
    "                int_ids = [int(x) for x in ids]\n",
    "                batch_inputs.append(int_ids)\n",
    "            batch_inputs = torch.tensor(batch_inputs).long().to(device)\n",
    "\n",
    "            #  forward pass\n",
    "            outputs = model.forward(input_ids=batch_inputs, labels=batch_inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            #  get loss\n",
    "            if multi_gpu:\n",
    "                loss = loss.mean()\n",
    "            if gradient_accumulation > 1:\n",
    "                loss = loss / gradient_accumulation\n",
    "\n",
    "            #  loss backward\n",
    "            #if fp16:\n",
    "            #    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            #        scaled_loss.backward()\n",
    "            #        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
    "            #else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            #  optimizer step\n",
    "            if (overall_step + 1) % gradient_accumulation == 0:\n",
    "                running_loss += loss.item()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            if (overall_step + 1) % log_step == 0:\n",
    "                #tb_writer.add_scalar('loss', loss.item() * gradient_accumulation, overall_step)\n",
    "                print('now time: {}:{}. Step {} of piece {} of epoch {}, loss {}'.format(\n",
    "                    datetime.now().hour,\n",
    "                    datetime.now().minute,\n",
    "                    step + 1,\n",
    "                    piece_num,\n",
    "                    epoch + 1,\n",
    "                    running_loss * gradient_accumulation / (log_step / gradient_accumulation)))\n",
    "                running_loss = 0\n",
    "            overall_step += 1\n",
    "        piece_num += 1\n",
    "\n",
    "    print('saving model for epoch {}'.format(epoch + 1))\n",
    "    if not os.path.exists(output_dir + 'model_epoch{}'.format(epoch + 1)):\n",
    "        os.mkdir(output_dir + 'model_epoch{}'.format(epoch + 1))\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(output_dir + 'model_epoch{}'.format(epoch + 1))\n",
    "    # torch.save(scheduler.state_dict(), output_dir + 'model_epoch{}/scheduler.pt'.format(epoch + 1))\n",
    "    # torch.save(optimizer.state_dict(), output_dir + 'model_epoch{}/optimizer.pt'.format(epoch + 1))\n",
    "    print('epoch {} finished'.format(epoch + 1))\n",
    "\n",
    "    then = datetime.now()\n",
    "    print('time: {}'.format(then))\n",
    "    print('time for one epoch: {}'.format(then - now))\n",
    "\n",
    "print('training finished')\n",
    "if not os.path.exists(output_dir + 'final_model'):\n",
    "    os.mkdir(output_dir + 'final_model')\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir + 'final_model')\n",
    "# torch.save(scheduler.state_dict(), output_dir + 'final_model/scheduler.pt')\n",
    "# torch.save(optimizer.state_dict(), output_dir + 'final_model/optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocab size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        # torch.topk()返回最后一维最大的top_k个元素，返回值为二维(values,indices)\n",
    "        # ...表示其他维度由计算机自行推断\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value  # 对于topk之外的其他元素的logits值设为负无穷\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)  # 对logits进行递减排序\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始和chatbot聊天，输入CTRL + Z以退出\n",
      "chatbot:我也要上##班！！\n",
      "chatbot:我也要上##班\n",
      "chatbot:可##惜我沒有我！\n",
      "chatbot:嘿##嘿。那就一樣的~\n",
      "chatbot:我不是說你這##麼有錢還是那麼有錢嗎？\n",
      "chatbot:謝##謝，我就不是一個有錢人。\n",
      "chatbot:我們就不用理##解你的好啦~\n",
      "chatbot:我們就不會理##解你的好啦~~\n",
      "chatbot:嘿##嘿！\n",
      "chatbot:哈##哈，我們都是好姐##們\n",
      "chatbot:\n",
      "chatbot:那你就別介樣##子了，我是很喜##歡你這##個樣##子的，哈##哈\n",
      "chatbot:你妹，你這##個是在侮##辱我們小女##孩的智##慧\n",
      "chatbot:你妹，我說我妹，我說你妹啊\n",
      "chatbot:你妹你妹啊你！！\n",
      "chatbot:那你還是去死吧\n",
      "chatbot:我還沒喫呢，那你去死吧\n",
      "chatbot:我還沒喫呢，你怎麼那麼不開##心呢\n",
      "chatbot:哈##哈！不用，你別欺##負你\n",
      "chatbot:那你也別欺##負我，哈##哈，你要欺##負我啊你\n",
      "chatbot:那我就別欺##負我\n",
      "chatbot:呵##呵，我們是朋##友吧\n",
      "chatbot:你懂的哈~\n",
      "chatbot:這##個你懂的~\n",
      "chatbot:我覺得你這##麼好，怎麼這##麼不開##心~你就給我們這些小姑\n",
      "chatbot:呵##呵！\n",
      "chatbot:哈##哈，你的微##博還是很漂##亮的。\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "max_history_len = 3\n",
    "max_len = 25\n",
    "temperature = 1\n",
    "repetition_penalty = 1\n",
    "topk = 8\n",
    "topp = 0\n",
    "cuda = False\n",
    "def main():\n",
    "    #args = set_args()\n",
    "    #logger = create_logger(args)\n",
    "    # 当用户使用GPU,并且GPU可用时\n",
    "    #args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
    "    device = 'cuda' if cuda else 'cpu'\n",
    "    #logger.info('using device:{}'.format(device))\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device\n",
    "\n",
    "    tokenizer = torch.load(\"manmade/tokenizer.ckpt\", map_location=torch.device('cpu'))\n",
    "    # tokenizer = BertTokenizer(vocab_file=args.voca_path)\n",
    "    model = transformers.modeling_gpt2.GPT2LMHeadModel.from_pretrained(\"./model_1/\")  # load pretrained model\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    #if args.save_samples_path:\n",
    "    #    if not os.path.exists(args.save_samples_path):\n",
    "    #        os.makedirs(args.save_samples_path)\n",
    "    #    samples_file = open(args.save_samples_path + '/samples.txt', 'a', encoding='utf8')\n",
    "    #    samples_file.write(\"聊天记录{}:\\n\".format(datetime.now()))\n",
    "    # 存储聊天记录，每个utterance以token的id的形式进行存储\n",
    "    history = []\n",
    "    print('开始和chatbot聊天，输入CTRL + Z以退出')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            text = input(\"user:\")\n",
    "            # text = \"你好\"\n",
    "            #if args.save_samples_path:\n",
    "            #    samples_file.write(\"user:{}\\n\".format(text))\n",
    "            text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "            history.append(text_ids)\n",
    "            input_ids = [tokenizer.convert_tokens_to_ids('[MASK]')]  # 每个input以[MASK]为开头\n",
    "            for history_id, history_utr in enumerate(history[-max_history_len:]):\n",
    "                input_ids.extend(history_utr)\n",
    "                input_ids.append(tokenizer.sep_token_id)\n",
    "            input_ids = torch.tensor(input_ids).long().to(device)\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "            response = []  # 根据context，生成的response\n",
    "            # 最多生成max_len个token\n",
    "            for _ in range(max_len):\n",
    "                outputs = model(input_ids=input_ids)\n",
    "                #print(input_ids)\n",
    "                #logits = outputs\n",
    "                next_token_logits = outputs[0][0, -1, :] # logtis 是 [0] [0 因為只有一筆 in batch, -1 是句子最後的位置 (代表每個位置其實都會生成), : 代表 vocab 大小的 prob distribution]\n",
    "                # 对于已生成的结果generated中的每个token添加一个重复惩罚项，降低其生成概率\n",
    "                for id in set(response):\n",
    "                    next_token_logits[id] /= repetition_penalty\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                # 对于[UNK]的概率设为无穷小，也就是说模型的预测结果不可能是[UNK]这个token\n",
    "                next_token_logits[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')\n",
    "                filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=topk, top_p=topp)\n",
    "                # torch.multinomial表示从候选集合中无放回地进行抽取num_samples个元素，权重越高，抽到的几率越高，返回元素的下标\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "                if next_token == tokenizer.cls_token_id:  # 遇到[cls]则表明response生成结束\n",
    "                    break\n",
    "                response.append(next_token.item())\n",
    "                input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "                # his_text = tokenizer.convert_ids_to_tokens(curr_input_tensor.tolist())\n",
    "                # print(\"his_text:{}\".format(his_text))\n",
    "            history.append(response)\n",
    "            text = tokenizer.convert_ids_to_tokens(response)\n",
    "            print(\"chatbot:\" + \"\".join(text).replace(\"#\", \"\"))\n",
    "            #if args.save_samples_path:\n",
    "            #    samples_file.write(\"chatbot:{}\\n\".format(\"\".join(text)))\n",
    "        except KeyboardInterrupt:\n",
    "            #if args.save_samples_path:\n",
    "            #    samples_file.close()\n",
    "            break\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdasd'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asd###asd'.replace(\"#\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7 > 6). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot:這##個事你們懂的[CLS][MASK]我不知##道怎麼回##來，就想回家了。[喜樂]\n"
     ]
    }
   ],
   "source": [
    "text = (\"今天又下雨了[噁心]\")\n",
    "history = []\n",
    "device = 'cpu'\n",
    "model = transformers.modeling_gpt2.GPT2LMHeadModel.from_pretrained(\"./model_1/\")  # load pretrained model\n",
    "# text = \"你好\"\n",
    "#if args.save_samples_path:\n",
    "#    samples_file.write(\"user:{}\\n\".format(text))\n",
    "text_ids = tokenizer.encode(text)\n",
    "history.append(text_ids)\n",
    "input_ids = [tokenizer.convert_tokens_to_ids('[MASK]')]  # 每个input以[MASK]为开头\n",
    "for history_id, history_utr in enumerate(history[-max_history_len:]):\n",
    "    input_ids.extend(history_utr)\n",
    "    input_ids.append(tokenizer.sep_token_id)\n",
    "input_ids = torch.tensor(input_ids).long().to(device)\n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "response = []  # 根据context，生成的response\n",
    "# 最多生成max_len个token\n",
    "for _ in range(max_len):\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    #print(type(outputs), len(outputs))\n",
    "    #print(input_ids)\n",
    "    #loss, logits = outputs[:2]\n",
    "    #logits = outputs\n",
    "    next_token_logits = outputs[0][0, -1, :]\n",
    "    # 对于已生成的结果generated中的每个token添加一个重复惩罚项，降低其生成概率\n",
    "    for id in set(response):\n",
    "        next_token_logits[id] /= repetition_penalty\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "    # 对于[UNK]的概率设为无穷小，也就是说模型的预测结果不可能是[UNK]这个token\n",
    "    next_token_logits[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')\n",
    "    filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=topk, top_p=topp)\n",
    "    # torch.multinomial表示从候选集合中无放回地进行抽取num_samples个元素，权重越高，抽到的几率越高，返回元素的下标\n",
    "    next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "    if next_token == tokenizer.sep_token_id:  # 遇到[SEP]则表明response生成结束\n",
    "        break\n",
    "    response.append(next_token.item())\n",
    "    input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "    # his_text = tokenizer.convert_ids_to_tokens(curr_input_tensor.tolist())\n",
    "    # print(\"his_text:{}\".format(his_text))\n",
    "history.append(response)\n",
    "text = tokenizer.convert_ids_to_tokens(response)\n",
    "print(\"chatbot:\" + \"\".join(text))\n",
    "#if args.save_samples_path:\n",
    "#    samples_file.write(\"chatbot:{}\\n\".format(\"\".join(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"num_labels\": 1,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pruned_heads\": {},\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 400\n",
       "    }\n",
       "  },\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"torchscript\": false,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 21134\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.modeling_gpt2.GPT2LMHeadModel.from_pretrained(\"./model_1/\")\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load(\"manmade/tokenizer.ckpt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]',\n",
       " 'additional_special_tokens': ['[其他]', '[喜歡]', '[悲傷]', '[噁心]', '[憤怒]', '[喜樂]']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map\n",
    "\n",
    "# [MASK] 開頭 103 tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "# [CLS] 結尾 101 tokenizer.convert_tokens_to_ids('[CLS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21128, 782, 6963, 2523, 1962]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('[其他]人都很好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chattwo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e64eee62b8c37dc64b8b380d3356b3dc4e38b0e834002de71da9f8165807feaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
